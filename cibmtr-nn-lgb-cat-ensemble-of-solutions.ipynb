{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:06:26.842696Z",
     "iopub.status.busy": "2025-02-16T03:06:26.842419Z",
     "iopub.status.idle": "2025-02-16T03:06:31.009182Z",
     "shell.execute_reply": "2025-02-16T03:06:31.008165Z",
     "shell.execute_reply.started": "2025-02-16T03:06:26.842673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def get_X_cat(df, cat_cols, transformers=None):\n",
    "    \"\"\"\n",
    "    Apply a specific categorical data transformer or a LabelEncoder if None.\n",
    "    \"\"\"\n",
    "    if transformers is None:\n",
    "        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n",
    "    return transformers, np.array(\n",
    "        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n",
    "    ).T\n",
    "\n",
    "\n",
    "def preprocess_data(train, val):\n",
    "    \"\"\"\n",
    "    Standardize numerical variables and transform (Label-encode) categoricals.\n",
    "    Fill NA values with mean for numerical.\n",
    "    Create torch dataloaders to prepare data for training and evaluation.\n",
    "    \"\"\"\n",
    "    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n",
    "    scaler = StandardScaler()\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "    X_num_train = imp.fit_transform(train[numerical])\n",
    "    X_num_train = scaler.fit_transform(X_num_train)\n",
    "    X_num_val = imp.transform(val[numerical])\n",
    "    X_num_val = scaler.transform(X_num_val)\n",
    "    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n",
    "    dl_val = init_dl(X_cat_val, X_num_val, val)\n",
    "    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n",
    "\n",
    "\n",
    "def get_categoricals(train, val):\n",
    "    \"\"\"\n",
    "    Remove constant categorical columns and transform them using LabelEncoder.\n",
    "    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n",
    "    \"\"\"\n",
    "    categorical_cols, numerical = get_feature_types(train)\n",
    "    remove = []\n",
    "    for col in categorical_cols:\n",
    "        if train[col].nunique() == 1:\n",
    "            remove.append(col)\n",
    "        ind = ~val[col].isin(train[col])\n",
    "        if ind.any():\n",
    "            val.loc[ind, col] = np.nan\n",
    "    categorical_cols = [col for col in categorical_cols if col not in remove]\n",
    "    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n",
    "    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n",
    "    return X_cat_train, X_cat_val, numerical, transformers\n",
    "\n",
    "\n",
    "def init_dl(X_cat, X_num, df, training=False):\n",
    "    \"\"\"\n",
    "    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n",
    "    Notice that efs_time is log-transformed.\n",
    "    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n",
    "    \"\"\"\n",
    "    ds_train = TensorDataset(\n",
    "        torch.tensor(X_cat, dtype=torch.long),\n",
    "        torch.tensor(X_num, dtype=torch.float32),\n",
    "        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n",
    "        torch.tensor(df.efs.values, dtype=torch.long)\n",
    "    )\n",
    "    bs = 2048\n",
    "    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n",
    "    return dl_train\n",
    "\n",
    "\n",
    "def get_feature_types(train):\n",
    "    \"\"\"\n",
    "    Utility function to return categorical and numerical column names.\n",
    "    \"\"\"\n",
    "    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n",
    "    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "    FEATURES = [c for c in train.columns if not c in RMV]\n",
    "    print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n",
    "    numerical = [i for i in FEATURES if i not in categorical_cols]\n",
    "    return categorical_cols, numerical\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Create some new features to help the model focus on specific patterns.\n",
    "    \"\"\"\n",
    "    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n",
    "    df['year_hct'] -= 2000\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load data and add features.\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "    test = add_features(test)\n",
    "    print(\"Test shape:\", test.shape)\n",
    "    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "    train = add_features(train)\n",
    "    print(\"Train shape:\", train.shape)\n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:06:31.011723Z",
     "iopub.status.busy": "2025-02-16T03:06:31.011316Z",
     "iopub.status.idle": "2025-02-16T03:15:35.863374Z",
     "shell.execute_reply": "2025-02-16T03:15:35.862576Z",
     "shell.execute_reply.started": "2025-02-16T03:06:31.0117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from pytorch_lightning.cli import ReduceLROnPlateau\n",
    "from pytorch_tabular.models.common.layers import ODST\n",
    "from torch import nn\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "\n",
    "\n",
    "class CatEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module for the categorical dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        projection_dim: int,\n",
    "        categorical_cardinality: List[int],\n",
    "        embedding_dim: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n",
    "        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n",
    "        embedding_dim: The size of the embedding space for each categorical feature.\n",
    "        self.embeddings: list of embedding layers for each categorical feature.\n",
    "        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n",
    "        \"\"\"\n",
    "        super(CatEmbeddings, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, embedding_dim)\n",
    "            for cardinality in categorical_cardinality\n",
    "        ])\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"\n",
    "        Apply the projection on concatened embeddings that contains all categorical features.\n",
    "        \"\"\"\n",
    "        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=1)\n",
    "        return self.projection(x_cat)\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    \"\"\"\n",
    "    Train a model on both categorical embeddings and numerical data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_dim: int,\n",
    "            categorical_cardinality: List[int],\n",
    "            embedding_dim: int,\n",
    "            projection_dim: int,\n",
    "            hidden_dim: int,\n",
    "            dropout: float = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        continuous_dim: The number of continuous features.\n",
    "        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n",
    "        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n",
    "        projection_dim: The size of the projected output space for the categorical embeddings.\n",
    "        hidden_dim: The number of neurons in the hidden layer of the MLP.\n",
    "        dropout: The dropout rate applied in the network.\n",
    "        self.embeddings: previous embeddings for categorical data.\n",
    "        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n",
    "        self.out: linear output layer that maps the output of the MLP to a single value\n",
    "        self.dropout: defines dropout\n",
    "        Weights initialization with xavier normal algorithm and biases with zeros.\n",
    "        \"\"\"\n",
    "        super(NN, self).__init__()\n",
    "        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            ODST(projection_dim + continuous_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Create embedding layers for categorical data, concatenate with continous variables.\n",
    "        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x_cat)\n",
    "        x = torch.cat([x, x_cont], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x), x\n",
    "\n",
    "\n",
    "@functools.lru_cache\n",
    "def combinations(N):\n",
    "    \"\"\"\n",
    "    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n",
    "    and caches the result using functools.lru_cache for optimization\n",
    "    \"\"\"\n",
    "    ind = torch.arange(N)\n",
    "    comb = torch.combinations(ind, r=2)\n",
    "    return comb.cuda()\n",
    "\n",
    "\n",
    "class LitNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Main Model creation and losses definition to fully train the model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_dim: int,\n",
    "            categorical_cardinality: List[int],\n",
    "            embedding_dim: int,\n",
    "            projection_dim: int,\n",
    "            hidden_dim: int,\n",
    "            lr: float = 1e-3,\n",
    "            dropout: float = 0.2,\n",
    "            weight_decay: float = 1e-3,\n",
    "            aux_weight: float = 0.1,\n",
    "            margin: float = 0.5,\n",
    "            race_index: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        continuous_dim: The number of continuous input features.\n",
    "        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n",
    "        embedding_dim: The dimension of the embeddings for the categorical features.\n",
    "        projection_dim: The dimension of the projected space after embedding concatenation.\n",
    "        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n",
    "        lr: The learning rate for the optimizer.\n",
    "        dropout: Dropout probability to avoid overfitting.\n",
    "        weight_decay: The L2 regularization term for the optimizer.\n",
    "        aux_weight: Weight used for auxiliary tasks.\n",
    "        margin: Margin used in some loss functions.\n",
    "        race_index: An index that refer to race_group in the input data.\n",
    "        \"\"\"\n",
    "        super(LitNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Creates an instance of the NN model defined above\n",
    "        self.model = NN(\n",
    "            continuous_dim=self.hparams.continuous_dim,\n",
    "            categorical_cardinality=self.hparams.categorical_cardinality,\n",
    "            embedding_dim=self.hparams.embedding_dim,\n",
    "            projection_dim=self.hparams.projection_dim,\n",
    "            hidden_dim=self.hparams.hidden_dim,\n",
    "            dropout=self.hparams.dropout\n",
    "        )\n",
    "        self.targets = []\n",
    "\n",
    "        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n",
    "        self.aux_cls = nn.Sequential(\n",
    "            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hparams.hidden_dim // 3, 1)\n",
    "        )\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        \"\"\"\n",
    "        Compute the 2-norm for each layer\n",
    "        If using mixed precision, the gradients are already unscaled here\n",
    "        \"\"\"\n",
    "        norms = grad_norm(self.model, norm_type=2)\n",
    "        self.log_dict(norms)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n",
    "        \"\"\"\n",
    "        x, emb = self.model(x_cat, x_cont)\n",
    "        return x.squeeze(1), emb\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        defines how the model processes each batch of data during training.\n",
    "        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n",
    "        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n",
    "        Calculates loss and race_group loss on full data.\n",
    "        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n",
    "        Returns loss and aux_loss multiplied by weight defined above.\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        aux_pred = self.aux_cls(emb).squeeze(1)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n",
    "        aux_mask = efs == 1\n",
    "        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss + aux_loss * self.hparams.aux_weight\n",
    "\n",
    "    def get_full_loss(self, efs, x_cat, y, y_hat):\n",
    "        \"\"\"\n",
    "        Output loss and race_group loss.\n",
    "        \"\"\"\n",
    "        loss = self.calc_loss(y, y_hat, efs)\n",
    "        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n",
    "        loss += 0.1 * race_loss\n",
    "        return loss, race_loss\n",
    "\n",
    "    def get_race_losses(self, efs, x_cat, y, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate loss for each race_group based on deviation/variance.\n",
    "        \"\"\"\n",
    "        races = torch.unique(x_cat[:, self.hparams.race_index])\n",
    "        race_losses = []\n",
    "        for race in races:\n",
    "            ind = x_cat[:, self.hparams.race_index] == race\n",
    "            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n",
    "        race_loss = sum(race_losses) / len(race_losses)\n",
    "        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n",
    "        return torch.sqrt(races_loss_std)\n",
    "\n",
    "    def calc_loss(self, y, y_hat, efs):\n",
    "        \"\"\"\n",
    "        Most important part of the model : loss function used for training.\n",
    "        We face survival data with event indicators along with time-to-event.\n",
    "\n",
    "        This function computes the main loss by the following the steps :\n",
    "        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n",
    "        * make sure that we have at least 1 event in each pair\n",
    "        * convert y to +1 or -1 depending on the correct ranking\n",
    "        * loss is computed using a margin-based hinge loss\n",
    "        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n",
    "        * average loss on all pairs is returned\n",
    "        \"\"\"\n",
    "        N = y.shape[0]\n",
    "        comb = combinations(N)\n",
    "        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n",
    "        pred_left = y_hat[comb[:, 0]]\n",
    "        pred_right = y_hat[comb[:, 1]]\n",
    "        y_left = y[comb[:, 0]]\n",
    "        y_right = y[comb[:, 1]]\n",
    "        y = 2 * (y_left > y_right).int() - 1\n",
    "        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n",
    "        mask = self.get_mask(comb, efs, y_left, y_right)\n",
    "        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def get_mask(self, comb, efs, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Defines all invalid comparisons :\n",
    "        * Case 1: \"Left outlived Right\" but Right is censored\n",
    "        * Case 2: \"Right outlived Left\" but Left is censored\n",
    "        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n",
    "        \"\"\"\n",
    "        left_outlived = y_left >= y_right\n",
    "        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n",
    "        mask2 = (left_outlived & left_1_right_0)\n",
    "        right_outlived = y_right >= y_left\n",
    "        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n",
    "        mask2 |= (right_outlived & right_1_left_0)\n",
    "        mask2 = ~mask2\n",
    "        mask = mask2\n",
    "        return mask\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        This method defines how the model processes each batch during validation\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        At the end of the validation epoch, it computes and logs the concordance index\n",
    "        \"\"\"\n",
    "        cindex, metric = self._calc_cindex()\n",
    "        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.targets.clear()\n",
    "\n",
    "    def _calc_cindex(self):\n",
    "        \"\"\"\n",
    "        Calculate c-index accounting for each race_group or global.\n",
    "        \"\"\"\n",
    "        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n",
    "        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n",
    "        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n",
    "        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n",
    "        metric = self._metric(efs, races, y, y_hat)\n",
    "        cindex = concordance_index(y, y_hat, efs)\n",
    "        return cindex, metric\n",
    "\n",
    "    def _metric(self, efs, races, y, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate c-index accounting for each race_group\n",
    "        \"\"\"\n",
    "        metric_list = []\n",
    "        for race in np.unique(races):\n",
    "            y_ = y[races == race]\n",
    "            y_hat_ = y_hat[races == race]\n",
    "            efs_ = efs[races == race]\n",
    "            metric_list.append(concordance_index(y_, y_hat_, efs_))\n",
    "        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n",
    "        return metric\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Same as training step but to log test data\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        At the end of the test epoch, calculates and logs the concordance index for the test set\n",
    "        \"\"\"\n",
    "        cindex, metric = self._calc_cindex()\n",
    "        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.targets.clear()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        configures the optimizer and learning rate scheduler:\n",
    "        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n",
    "        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=45,\n",
    "                eta_min=6e-3\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": False,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "def main(hparams):\n",
    "    \"\"\"\n",
    "    Main function to train the model.\n",
    "    The steps are as following :\n",
    "    * load data and fill efs and efs time for test data with 1\n",
    "    * initialize pred array with 0\n",
    "    * get categorical and numerical columns\n",
    "    * split the train data on the stratified criterion : race_group * newborns yes/no\n",
    "    * preprocess the fold data (create dataloaders)\n",
    "    * train the model and create final submission output\n",
    "    \"\"\"\n",
    "    test, train_original = load_data()\n",
    "    test['efs_time'] = 1\n",
    "    test['efs'] = 1\n",
    "    test_pred = np.zeros(test.shape[0])\n",
    "    categorical_cols, numerical = get_feature_types(train_original)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, )\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        kf.split(\n",
    "            train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str)\n",
    "        )\n",
    "    ):\n",
    "        tt = train_original.copy()\n",
    "        train = tt.iloc[train_index]\n",
    "        val = tt.iloc[test_index]\n",
    "        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n",
    "        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols)\n",
    "        # Create submission\n",
    "        train = tt.iloc[train_index]\n",
    "        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n",
    "        pred, _ = model.cuda().eval()(\n",
    "            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n",
    "            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n",
    "        )\n",
    "        test_pred += pred.detach().cpu().numpy()\n",
    "        \n",
    "    subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "    subm_data['prediction'] = -test_pred\n",
    "    subm_data.to_csv('submission1.csv', index=False)\n",
    "    \n",
    "    display(subm_data.head())\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "def train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Defines model hyperparameters and fit the model.\n",
    "    \"\"\"\n",
    "    if hparams is None:\n",
    "        hparams = {\n",
    "            \"embedding_dim\": 16,\n",
    "            \"projection_dim\": 112,\n",
    "            \"hidden_dim\": 56,\n",
    "            \"lr\": 0.06464861983337984,\n",
    "            \"dropout\": 0.05463240181423116,\n",
    "            \"aux_weight\": 0.26545778308743806,\n",
    "            \"margin\": 0.2588153271003354,\n",
    "            \"weight_decay\": 0.0002773544957610778\n",
    "        }\n",
    "    model = LitNN(\n",
    "        continuous_dim=X_num_train.shape[1],\n",
    "        categorical_cardinality=[len(t.classes_) for t in transformers],\n",
    "        race_index=categorical_cols.index(\"race_group\"),\n",
    "        **hparams\n",
    "    )\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='cuda',\n",
    "        max_epochs=60,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            LearningRateMonitor(logging_interval='epoch'),\n",
    "            TQDMProgressBar(),\n",
    "            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n",
    "        ],\n",
    "    )\n",
    "    trainer.fit(model, dl_train)\n",
    "    trainer.test(model, dl_val)\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "hparams = None\n",
    "res = main(hparams)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB & CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:15:35.865268Z",
     "iopub.status.busy": "2025-02-16T03:15:35.86496Z",
     "iopub.status.idle": "2025-02-16T03:15:35.877922Z",
     "shell.execute_reply": "2025-02-16T03:15:35.877029Z",
     "shell.execute_reply.started": "2025-02-16T03:15:35.865245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\n",
    "target_file_path = '/kaggle/working/baseline.py'\n",
    "with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "with open(target_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:15:35.879128Z",
     "iopub.status.busy": "2025-02-16T03:15:35.878832Z",
     "iopub.status.idle": "2025-02-16T03:15:40.143514Z",
     "shell.execute_reply": "2025-02-16T03:15:40.142156Z",
     "shell.execute_reply.started": "2025-02-16T03:15:35.879104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n",
    "--no-index --find-links file:/kaggle/input/yunbase/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:15:40.152325Z",
     "iopub.status.busy": "2025-02-16T03:15:40.152022Z",
     "iopub.status.idle": "2025-02-16T03:16:13.235086Z",
     "shell.execute_reply": "2025-02-16T03:16:13.234362Z",
     "shell.execute_reply.started": "2025-02-16T03:15:40.152291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from baseline import Yunbase\n",
    "import pandas as pd#read csv,parquet\n",
    "import numpy as np#for scientific computation of matrices\n",
    "from  lightgbm import LGBMRegressor,LGBMClassifier,log_evaluation,early_stopping\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from lifelines import KaplanMeierFitter\n",
    "import warnings#avoid some negligible errors\n",
    "#The filterwarnings () method is used to set warning filters, which can control the output method and level of warning information.\n",
    "warnings.filterwarnings('ignore')\n",
    "import random#provide some function to generate random_seed.\n",
    "#set random seed,to make sure model can be recurrented.\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)#numpy's random seed\n",
    "    random.seed(seed)#python built-in random seed\n",
    "seed_everything(seed=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:16:13.239013Z",
     "iopub.status.busy": "2025-02-16T03:16:13.238054Z",
     "iopub.status.idle": "2025-02-16T03:16:13.61889Z",
     "shell.execute_reply": "2025-02-16T03:16:13.618141Z",
     "shell.execute_reply.started": "2025-02-16T03:16:13.238985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "train_solution=train[['ID','efs','efs_time','race_group']].copy()\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "max_efs_time,min_efs_time=80,-100\n",
    "train['efs_time']=train['efs_time']/(max_efs_time-min_efs_time)\n",
    "train['efs_time']=train['efs_time'].apply(lambda x:logit(x))\n",
    "train['efs_time']+=10\n",
    "print(train['efs_time'].max(),train['efs_time'].min())\n",
    "\n",
    "race2weight={'American Indian or Alaska Native':0.68,\n",
    "'Asian':0.7,'Black or African-American':0.67,\n",
    "'More than one race':0.68,\n",
    "'Native Hawaiian or other Pacific Islander':0.66,\n",
    "'White':0.64}\n",
    "train['weight']=0.5*train['efs']+0.5\n",
    "train['raceweight']=train['race_group'].apply(lambda x:race2weight.get(x,1))\n",
    "train['weight']=train['weight']/train['raceweight']\n",
    "train.drop(['raceweight'],axis=1,inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:16:13.620406Z",
     "iopub.status.busy": "2025-02-16T03:16:13.620147Z",
     "iopub.status.idle": "2025-02-16T03:16:14.191833Z",
     "shell.execute_reply": "2025-02-16T03:16:14.191063Z",
     "shell.execute_reply.started": "2025-02-16T03:16:13.620383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    kmf.fit(df[time_col], event_observed=df[event_col])\n",
    "    \n",
    "    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n",
    "\n",
    "    return survival_probabilities\n",
    "\n",
    "race_group=sorted(train['race_group'].unique())\n",
    "for race in race_group:\n",
    "    train.loc[train['race_group']==race,\"target\"] = transform_survival_probability(train[train['race_group']==race], time_col='efs_time', event_col='efs')\n",
    "    gap=0.7*(train.loc[(train['race_group']==race)&(train['efs']==0)]['target'].max()-train.loc[(train['race_group']==race)&(train['efs']==1)]['target'].min())/2\n",
    "    train.loc[(train['race_group']==race)&(train['efs']==0),'target']-=gap\n",
    "\n",
    "sns.histplot(data=train, x='target', hue='efs', element='step', stat='density', common_norm=False)\n",
    "plt.legend(title='efs')\n",
    "plt.title('Distribution of Target by EFS')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "train.drop(['efs','efs_time'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:16:14.198786Z",
     "iopub.status.busy": "2025-02-16T03:16:14.198505Z",
     "iopub.status.idle": "2025-02-16T03:55:02.949326Z",
     "shell.execute_reply": "2025-02-16T03:55:02.948381Z",
     "shell.execute_reply.started": "2025-02-16T03:16:14.198756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#nunique=2\n",
    "nunique2=[col for col in train.columns if train[col].nunique()==2 and col!='efs']\n",
    "#nunique<50\n",
    "nunique50=[col for col in train.columns if train[col].nunique()<50 and col not in ['efs','weight']]+['age_group','dri_score_NA']\n",
    "\n",
    "def FE(df):\n",
    "    print(\"< deal with outlier >\")\n",
    "    df['nan_value_each_row'] = df.isnull().sum(axis=1)\n",
    "    #year_hct=2020 only 4 rows.\n",
    "    df['year_hct']=df['year_hct'].replace(2020,2019)\n",
    "    df['age_group']=df['age_at_hct']//10\n",
    "    #karnofsky_score 40 only 10 rows.\n",
    "    df['karnofsky_score']=df['karnofsky_score'].replace(40,50)\n",
    "    #hla_high_res_8=2 only 2 rows.\n",
    "    df['hla_high_res_8']=df['hla_high_res_8'].replace(2,3)\n",
    "    #hla_high_res_6=0 only 1 row.\n",
    "    df['hla_high_res_6']=df['hla_high_res_6'].replace(0,2)\n",
    "    #hla_high_res_10=3 only 1 row.\n",
    "    df['hla_high_res_10']=df['hla_high_res_10'].replace(3,4)\n",
    "    #hla_low_res_8=2 only 1 row.\n",
    "    df['hla_low_res_8']=df['hla_low_res_8'].replace(2,3)\n",
    "    df['dri_score']=df['dri_score'].replace('Missing disease status','N/A - disease not classifiable')\n",
    "    df['dri_score_NA']=df['dri_score'].apply(lambda x:int('N/A' in str(x)))\n",
    "    for col in ['diabetes','pulm_moderate','cardiac']:\n",
    "        df.loc[df[col].isna(),col]='Not done'\n",
    "\n",
    "    print(\"< cross feature >\")\n",
    "    df['donor_age-age_at_hct']=df['donor_age']-df['age_at_hct']\n",
    "    df['comorbidity_score+karnofsky_score']=df['comorbidity_score']+df['karnofsky_score']\n",
    "    df['comorbidity_score-karnofsky_score']=df['comorbidity_score']-df['karnofsky_score']\n",
    "    df['comorbidity_score*karnofsky_score']=df['comorbidity_score']*df['karnofsky_score']\n",
    "    df['comorbidity_score/karnofsky_score']=df['comorbidity_score']/df['karnofsky_score']\n",
    "    \n",
    "    print(\"< fillna >\")\n",
    "    df[nunique50]=df[nunique50].astype(str).fillna('NaN')\n",
    "    \n",
    "    print(\"< combine category feature >\")\n",
    "    for i in range(len(nunique2)):\n",
    "        for j in range(i+1,len(nunique2)):\n",
    "            df[nunique2[i]+nunique2[j]]=df[nunique2[i]].astype(str)+df[nunique2[j]].astype(str)\n",
    "    \n",
    "    print(\"< drop useless columns >\")\n",
    "    df.drop(['ID'],axis=1,inplace=True,errors='ignore')\n",
    "    return df\n",
    "\n",
    "combine_category_cols=[]\n",
    "for i in range(len(nunique2)):\n",
    "    for j in range(i+1,len(nunique2)):\n",
    "        combine_category_cols.append(nunique2[i]+nunique2[j])  \n",
    "\n",
    "total_category_feature=nunique50+combine_category_cols\n",
    "\n",
    "target_stat=[]\n",
    "for j in range(len(total_category_feature)):\n",
    "   for col in ['donor_age','age_at_hct','target']:\n",
    "    target_stat.append( (total_category_feature[j],col,['count','mean','max','std','skew']) )\n",
    "\n",
    "num_folds=10\n",
    "\n",
    "lgb_params={\"boosting_type\": \"gbdt\",\"metric\": 'mae',\n",
    "            'random_state': 2025,  \"max_depth\": 9,\"learning_rate\": 0.1,\n",
    "            \"n_estimators\": 768,\"colsample_bytree\": 0.6,\"colsample_bynode\": 0.6,\n",
    "            \"verbose\": -1,\"reg_alpha\": 0.2,\n",
    "            \"reg_lambda\": 5,\"extra_trees\":True,'num_leaves':64,\"max_bin\":255,\n",
    "            'importance_type': 'gain',#better than 'split'\n",
    "            'device':'gpu','gpu_use_dp':True\n",
    "           }\n",
    "\n",
    "cat_params={'random_state':2025,'eval_metric' : 'MAE',\n",
    "            'bagging_temperature': 0.50,'iterations': 650,\n",
    "            'learning_rate': 0.1,'max_depth': 8,\n",
    "            'l2_leaf_reg': 1.25,'min_data_in_leaf': 24,\n",
    "            'random_strength' : 0.25, 'verbose': 0,\n",
    "            'task_type':'GPU',\n",
    "            }\n",
    "\n",
    "yunbase=Yunbase(num_folds=num_folds,\n",
    "                  models=[(LGBMRegressor(**lgb_params),'lgb'),\n",
    "                          (CatBoostRegressor(**cat_params),'cat')\n",
    "                         ],\n",
    "                  FE=FE,\n",
    "                  seed=2025,\n",
    "                  objective='regression',\n",
    "                  metric='mae',\n",
    "                  target_col='target',\n",
    "                  device='gpu',\n",
    "                  one_hot_max=-1,\n",
    "                  early_stop=1000,\n",
    "                  cross_cols=['donor_age','age_at_hct'],\n",
    "                  target_stat=target_stat,\n",
    "                  use_data_augmentation=True,\n",
    "                  use_scaler=True,\n",
    "                  log=250,\n",
    "                  plot_feature_importance=True,\n",
    "                  #print metric score when model training\n",
    "                  use_eval_metric=False,\n",
    ")\n",
    "yunbase.fit(train,category_cols=nunique2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:55:02.950603Z",
     "iopub.status.busy": "2025-02-16T03:55:02.950331Z",
     "iopub.status.idle": "2025-02-16T03:55:04.07741Z",
     "shell.execute_reply": "2025-02-16T03:55:04.076614Z",
     "shell.execute_reply.started": "2025-02-16T03:55:02.95058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas.api.types\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n",
    "\n",
    "weights = [0.602, 0.398]\n",
    "\n",
    "lgb_prediction=np.load(f\"Yunbase_info/lgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\n",
    "lgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':lgb_prediction})\n",
    "print(f\"lgb_score:{score(train_solution.copy(),lgb_prediction.copy(),row_id_column_name='ID')}\")\n",
    "cat_prediction=np.load(f\"Yunbase_info/cat_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\n",
    "cat_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':cat_prediction})\n",
    "print(f\"cat_score:{score(train_solution.copy(),cat_prediction.copy(),row_id_column_name='ID')}\")\n",
    "\n",
    "y_preds=[lgb_prediction.copy(),cat_prediction.copy()]\n",
    "final_prediction=lgb_prediction.copy()\n",
    "final_prediction['prediction']=0\n",
    "for i in range(len(y_preds)):\n",
    "    final_prediction['prediction']+=weights[i]*y_preds[i]['prediction']\n",
    "metric=score(train_solution.copy(),final_prediction.copy(),row_id_column_name='ID')\n",
    "print(f\"final_CV:{metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:55:04.078589Z",
     "iopub.status.busy": "2025-02-16T03:55:04.078313Z",
     "iopub.status.idle": "2025-02-16T03:58:22.319865Z",
     "shell.execute_reply": "2025-02-16T03:58:22.318951Z",
     "shell.execute_reply.started": "2025-02-16T03:55:04.078567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_preds=yunbase.predict(test,weights=weights)\n",
    "yunbase.target_col='prediction'\n",
    "yunbase.submit(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\",test_preds,\n",
    "               save_name='submission2'\n",
    "              )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219607918,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221291379,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
